"""
    对一个灰度图像张量的flatten操作
    以及如何展开特定的轴，即选择性的flatten操作
"""


# 卷积层输出在进入全连接层前，必须进行flatten操作

# flatten操作是一种特殊的reshape操作
# 将多维张量展开成一维张量
# 例如，将形状为(2, 3, 4)的张量展开成形状为(24,)的张量

import torch

# 创建三个4 * 4的张量，将它们看成是三张4 * 4的图像
# 被用来创建一个批次传入CNN
# 复习CNN
# CNN是卷积神经网络的缩写，是一种常用于图像识别和计算机视觉任务的深度学习模型
# CNN的输入通常是一个四维张量，形状为[batch_size, channels, height, width]
# batch_size表示输入的图像数量，channels表示图像的颜色通道数，height和width表示图像的高度和宽度

t1 = torch.tensor([
    [1, 1, 1, 1],
    [1, 1, 1, 1],
    [1, 1, 1, 1],
    [1, 1, 1, 1]
])

# print(t1.shape)

t2 = torch.tensor([
    [2, 2, 2, 2],
    [2, 2, 2, 2],
    [2, 2, 2, 2],
    [2, 2, 2, 2]
])

t3 = torch.tensor([
    [3, 3, 3, 3],
    [3, 3, 3, 3],
    [3, 3, 3, 3],
    [3, 3, 3, 3]
])

# 在CNN里使用一个张量表示的是一个完整的批次
# 所以我们需要将这三个张量合并成一个更大的张量
# 应该是三个轴，而不是两个


# stack()函数可以将多个张量沿着一个新的维度进行拼接
# 拼接后的张量的维度比原来的张量的维度多1
# 新的维度的大小就是拼接的张量的数量
# 例如，将三个形状为(2, 3)的张量沿着新的维度进行拼接，得到一个形状为(3, 2, 3)的张量

tt = torch.stack((t1, t2, t3))
print(tt)
print(tt.shape) # torch.Size([3, 4, 4]) 3代表的就是一批次的大小

# cat()函数也是用于拼接的，但是
# cat()函数可以将多个张量沿着一个已经存在的维度进行拼接
# 拼接后的张量的维度不变
# 例如，将三个形状为(2, 3)的张量沿着第一个维度进行拼接，得到一个形状为(6, 3)的张量

# 所以，stack()和cat()的区别在于拼接后张量的维度是否增加
# 我们需要将这三个张量合并成一个更大的张量，所以这里采用的是stack()函数，而不是cat()函数


# 目前我们拥有了一个三维张量，但是距离CNN的输入，我们还差了一个彩色通道的维度
# 我们可以使用unsqueeze()函数来添加一个新的维度，表示彩色通道
# 对于灰度图像，我们通常将其看作是只有一个通道的彩色图像，所以对于每个图像张量，我们基本都有一个隐式的单色通道
# 可以通过reshape()或者unsqueeze()来进行重塑
tt = tt.unsqueeze(1)
print(tt.shape)
# 或
tt = tt.reshape(3, 1, 4, 4)
print(tt.shape)
 # torch.Size([3, 1, 4, 4]) 1代表的就是彩色通道的大小，这里是1，表示灰度图像

# 从5分30秒开始至6分33秒的视频很重要！

print(tt[0])
print(tt[0][0])
print(tt[0][0][0])
print(tt[0][0][0][0])


# 在CNN里使用一个张量表示的是一个完整的批次
# 目前的tt张量满足CNN输入通常是一个四维张量，形状为[batch_size, channels, height, width]

# 我们首先将整个都flatten看看结果
print(tt.reshape(-1))
# 或
print(tt.flatten())

# 输出结果的1表示来自第一张图像，2来自第二个图像，3来自第三个图像
# 目前我们将所有的图像元素都映射到了一个轴上
# 但是我们的目标是对三个图进行单独的预测

# 这意味着我们只想把张量中的一部分变平，即从第二个维度开始flatten，跳过批次轴，从颜色通道轴开始
print(tt.flatten(start_dim=1).shape)
# torch.Size([3, 16]) 中，3还是第一个3，16 = 1 * 4 * 4
print(tt.flatten(start_dim=1))

# 同时我们也可以用reshape的方式来达到同样的效果
print(tt.reshape(3, 16).shape)
# 更简单的方法
print(tt.reshape(3, -1).shape)# -1指的是自动计算该维度的大小，以适应张量中的元素数量
# 这里的-1在从头开始实现flatten时出现过
# 从头开始实现flatten
def flatten(t):
    # 获取张量的元素数量
    t = t.reshape(1, -1) # -1指的是自动计算该维度的大小，以适应张量中的元素数量
    # 删除维度为1的轴
    t = t.squeeze()
    return t